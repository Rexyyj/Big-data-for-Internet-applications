{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Local vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,0.0,3.0]\n",
      "(3,[0,2],[1.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "dv = Vectors.dense(1.0,0.0,3.0)\n",
    "sv = Vectors.sparse(3,[0,2],[1.0,3.0])\n",
    "print(dv)\n",
    "print(sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Labeled points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,[2.0,5.0,3.0])\n",
      "(1.0,(3,[0,2],[1.0,3.0]))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "neg = LabeledPoint(0.0,[2.0,5.0,3.0])#the first value is a double-typed label\n",
    "pos = LabeledPoint(1.0,SparseVector(3,[0,2],[1.0,3.0]))\n",
    "print(neg)\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* local matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[9., 0.],\n",
      "             [0., 8.],\n",
      "             [0., 6.]])\n",
      "3 X 2 CSCMatrix\n",
      "(0,0) 9.0\n",
      "(2,1) 6.0\n",
      "(1,1) 8.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Matrices\n",
    "\n",
    "dm = Matrices.dense(3,2,[9,0,0,0,8,6])\n",
    "sm = Matrices.sparse(3,2,[0,1,3],[0,2,1],[9,6,8])#(row,column,[indexs of row],[index of columns],[index of values])\n",
    "\n",
    "print(dm)\n",
    "print(sm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spare labeled data (LIBSVM format)\n",
    "\n",
    "Each line's format: label index1:value1 index2:value2 ....\n",
    "(index start from 1,but after loaded, index -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# myDF= spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pipeline for estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "# Prepare training documents from a list of (id, text, label) tuples.\n",
    "training = spark.createDataFrame([\n",
    " (0, \"a b c d e spark\", 1.0),\n",
    " (1, \"b d\", 0.0),\n",
    " (2, \"spark f g h\", 1.0),\n",
    " (3, \"hadoop mapreduce\", 0.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "# Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.001)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "| id|              text|               words|            features|       rawPrediction|         probability|prediction|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  4|       spark i j k|    [spark, i, j, k]|(262144,[20197,24...|[-1.6609033227472...|[0.15964077387874...|       1.0|\n",
      "|  5|             l m n|           [l, m, n]|(262144,[18910,10...|[1.64218895265644...|[0.83783256854767...|       0.0|\n",
      "|  6|spark hadoop spark|[spark, hadoop, s...|(262144,[155117,2...|[-2.5980142174393...|[0.06926633132976...|       1.0|\n",
      "|  7|     apache hadoop|    [apache, hadoop]|(262144,[66695,15...|[4.00817033336812...|[0.98215753334442...|       0.0|\n",
      "+---+------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare test documents, which are unlabeled (id, text) tuples.\n",
    "test = spark.createDataFrame([\n",
    " (4, \"spark i j k\"),\n",
    " (5, \"l m n\"),\n",
    " (6, \"spark hadoop spark\"),\n",
    " (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "# Make predictions on test documents and print columns of interest.\n",
    "prediction = model.transform(test)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Vector Assembler (a transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+\n",
      "|colA|colB| colC|\n",
      "+----+----+-----+\n",
      "|   1| 4.5| true|\n",
      "|   2| 0.6| true|\n",
      "|   3| 1.5|false|\n",
      "|   4|12.1| true|\n",
      "|   5| 0.0| true|\n",
      "+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark2 = SparkSession.builder.getOrCreate()\n",
    "profilesList = [(1,4.5,True),(2,0.6,True),(3,1.5,False),(4,12.1,True),(5,0.0,True)]\n",
    "inputDF = spark2.createDataFrame(profilesList,['colA','colB','colC'])\n",
    "inputDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+--------------+\n",
      "|colA|colB| colC|      features|\n",
      "+----+----+-----+--------------+\n",
      "|   1| 4.5| true| [1.0,4.5,1.0]|\n",
      "|   2| 0.6| true| [2.0,0.6,1.0]|\n",
      "|   3| 1.5|false| [3.0,1.5,0.0]|\n",
      "|   4|12.1| true|[4.0,12.1,1.0]|\n",
      "|   5| 0.0| true| [5.0,0.0,1.0]|\n",
      "+----+----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "feat_cols = ['colA', 'colB', 'colC']\n",
    "vectorAssembler = VectorAssembler(inputCols = feat_cols,\n",
    "outputCol = 'features')\n",
    "transformedDF = vectorAssembler.transform(inputDF)\n",
    "transformedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Standard scaler (Estimator)\n",
    "\n",
    "transforms a dataset of Vector rows, normalizing each feature to have unit standard deviation and/or zero mean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----+--------------+--------------------+\n",
      "|colA|colB| colC|      features|      scaledFeatures|\n",
      "+----+----+-----+--------------+--------------------+\n",
      "|   1| 4.5| true| [1.0,4.5,1.0]|[-1.2649110640673...|\n",
      "|   2| 0.6| true| [2.0,0.6,1.0]|[-0.6324555320336...|\n",
      "|   3| 1.5|false| [3.0,1.5,0.0]|[0.0,-0.449503858...|\n",
      "|   4|12.1| true|[4.0,12.1,1.0]|[0.63245553203367...|\n",
      "|   5| 0.0| true| [5.0,0.0,1.0]|[1.26491106406735...|\n",
      "+----+----+-----+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scaler = StandardScaler(inputCol='features', outputCol=\"scaledFeatures\",\n",
    "withStd=True, withMean=True)\n",
    "# fitting the StandardScaler. Then Normalize each feature to have a unit standard deviation.\n",
    "scalerModel = scaler.fit(transformedDF)\n",
    "scaledDF = scalerModel.transform(transformedDF)\n",
    "scaledDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Tokenizer(transformer)\n",
    "Taking text (such as a sentence) and breaking it into individual terms (usually words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|            sentence|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|Hi I heard about ...|[hi, i, heard, ab...|\n",
      "|  1|I wish we can hav...|[i, wish, we, can...|\n",
      "+---+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "sentenceDF = spark.createDataFrame([\n",
    " (0, \"Hi I heard about Spark\"),\n",
    " (1, \"I wish we can have more Spark classes\"),\n",
    "], [\"id\", \"sentence\"])\n",
    "tokenizer = Tokenizer(inputCol=\"sentence\",\n",
    "outputCol=\"words\")\n",
    "tokenizedDF = tokenizer.transform(sentenceDF)\n",
    "tokenizedDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* StringIndexer / IndexToString(estimator)\n",
    "\n",
    "transformation of categorical class label into numerical one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+-----+\n",
      "|categoricalLabel|       features|label|\n",
      "+----------------+---------------+-----+\n",
      "|        Positive|[0.0, 1.1, 0.1]|  0.0|\n",
      "|        Negative|[0.1, 1.2, 0.2]|  1.0|\n",
      "|        Positive|[0.0, 1.3, 0.3]|  0.0|\n",
      "|        Negative|[0.3, 1.4, 0.4]|  1.0|\n",
      "|        Positive|[0.0, 1.4, 0.5]|  0.0|\n",
      "+----------------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "inputDF=spark.createDataFrame([\n",
    "    (\"Positive\",[0.0,1.1,0.1]), (\"Negative\",[0.1,1.2,0.2]), (\"Positive\",[0.0,1.3,0.3]), (\"Negative\",[0.3,1.4,0.4]), (\"Positive\",[0.0,1.4,0.5])\n",
    "],[\"categoricalLabel\",\"features\"])\n",
    "indexer = StringIndexer(inputCol=\"categoricalLabel\",outputCol=\"label\")\n",
    "indexerModel = indexer.fit(inputDF)\n",
    "indexedDF=indexerModel.transform(inputDF)\n",
    "indexedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|       features|label|\n",
      "+---------------+-----+\n",
      "|[0.0, 1.1, 0.1]|  0.0|\n",
      "|[0.1, 1.2, 0.2]|  1.0|\n",
      "|[0.0, 1.3, 0.3]|  0.0|\n",
      "|[0.3, 1.4, 0.4]|  1.0|\n",
      "|[0.0, 1.4, 0.5]|  0.0|\n",
      "+---------------+-----+\n",
      "\n",
      "+---------------+-----+-------------+\n",
      "|       features|label|originalLabel|\n",
      "+---------------+-----+-------------+\n",
      "|[0.0, 1.1, 0.1]|  0.0|     Positive|\n",
      "|[0.1, 1.2, 0.2]|  1.0|     Negative|\n",
      "|[0.0, 1.3, 0.3]|  0.0|     Positive|\n",
      "|[0.3, 1.4, 0.4]|  1.0|     Negative|\n",
      "|[0.0, 1.4, 0.5]|  0.0|     Positive|\n",
      "+---------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import IndexToString\n",
    "indexedDF2= indexedDF.select(\"features\",\"label\")\n",
    "indexedDF2.show()\n",
    "converter = IndexToString(inputCol=\"label\",outputCol=\"originalLabel\")\n",
    "convertedDF2 = converter.transform(indexedDF2)\n",
    "convertedDF2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+-------------+\n",
      "|Weather|Temperature|WeatherIndex|WeatherOneHot|\n",
      "+-------+-----------+------------+-------------+\n",
      "|    Fog|         30|         1.0|(2,[1],[1.0])|\n",
      "|   Rain|         25|         0.0|(2,[0],[1.0])|\n",
      "|    Sun|         36|         2.0|    (2,[],[])|\n",
      "+-------+-----------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "df = spark.createDataFrame([\n",
    " ('Fog', 30),\n",
    " ('Rain', 25),\n",
    " ('Sun',36),\n",
    "], [\"Weather\", \"Temperature\"])\n",
    "indexer = StringIndexer(inputCol=\"Weather\", outputCol=\"WeatherIndex\")\n",
    "\n",
    "indexerModel = indexer.fit(df)\n",
    "indexedDF=indexerModel.transform(df)\n",
    "encoder = OneHotEncoderEstimator(inputCols=[\"WeatherIndex\"],outputCols=[\"WeatherOneHot\"])\n",
    "model = encoder.fit(indexedDF)\n",
    "encodedDF = model.transform(indexedDF)\n",
    "encodedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
