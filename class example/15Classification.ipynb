{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Split data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "3\n",
      "2\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "inputDF=spark.createDataFrame([\n",
    "    (\"Positive\",[0.0,1.1,0.1]), (\"Negative\",[0.1,1.2,0.2]), (\"Positive\",[0.0,1.3,0.3]), (\"Negative\",[0.3,1.4,0.4]), (\"Positive\",[0.0,1.4,0.5])\n",
    "],[\"categoricalLabel\",\"features\"])\n",
    "print(inputDF.count())\n",
    "\n",
    "trainValidation,test = inputDF.randomSplit([0.75,0.25])\n",
    "print(trainValidation.count())\n",
    "print(test.count())\n",
    "print(trainValidation.count()+test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Classification**\n",
    "\n",
    "All the classification algorithms available in Spark work only on numerical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+\n",
      "|Savings|Income|Risk|\n",
      "+-------+------+----+\n",
      "|  15000|  1100| Low|\n",
      "|      0|  5000|High|\n",
      "|  20000|   800|High|\n",
      "|   6000|  1300| Low|\n",
      "|  50000|  2500| Low|\n",
      "|   2000|  1100| Low|\n",
      "|    700|  1500|High|\n",
      "|  75000|     0|High|\n",
      "|   4000|   500|High|\n",
      "+-------+------+----+\n",
      "\n",
      "+-------+------+----+\n",
      "|Savings|Income|Risk|\n",
      "+-------+------+----+\n",
      "| 100000| 10000| Low|\n",
      "|    100|   100|High|\n",
      "|   3100|   900|High|\n",
      "|   2000|  1500|High|\n",
      "|   3500|  1200| Low|\n",
      "+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data= spark.createDataFrame([\n",
    "    (15000,1100,\"Low\"),\n",
    "    (0,5000,\"High\"),\n",
    "    (20000,800,\"High\"),\n",
    "    (6000,1300,\"Low\"),\n",
    "    (50000,2500,\"Low\"),\n",
    "    (2000,1100,\"Low\"),\n",
    "    (700,1500,\"High\"),\n",
    "    (75000,0,\"High\"),\n",
    "    (4000,500,\"High\")\n",
    "],[\"Savings\",\"Income\",\"Risk\"])\n",
    "data.show()\n",
    "testData= spark.createDataFrame([\n",
    "    (100000,10000,\"Low\"),\n",
    "    (100,100,\"High\"),\n",
    "    (3100,900,\"High\"),\n",
    "    (2000,1500,\"High\"),\n",
    "    (3500,1200,\"Low\")\n",
    "],[\"Savings\",\"Income\",\"Risk\"])\n",
    "testData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+---------+----------------+\n",
      "|Savings|Income|Risk|RiskIndex|        features|\n",
      "+-------+------+----+---------+----------------+\n",
      "|  15000|  1100| Low|      1.0|[15000.0,1100.0]|\n",
      "|      0|  5000|High|      0.0|    [0.0,5000.0]|\n",
      "|  20000|   800|High|      0.0| [20000.0,800.0]|\n",
      "|   6000|  1300| Low|      1.0| [6000.0,1300.0]|\n",
      "|  50000|  2500| Low|      1.0|[50000.0,2500.0]|\n",
      "|   2000|  1100| Low|      1.0| [2000.0,1100.0]|\n",
      "|    700|  1500|High|      0.0|  [700.0,1500.0]|\n",
      "|  75000|     0|High|      0.0|   [75000.0,0.0]|\n",
      "|   4000|   500|High|      0.0|  [4000.0,500.0]|\n",
      "+-------+------+----+---------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "indexer = StringIndexer(inputCol=\"Risk\", outputCol=\"RiskIndex\",\n",
    "                        handleInvalid=\"keep\")\n",
    "indexerModel = indexer.fit(data)\n",
    "indexedDF=indexerModel.transform(data)\n",
    "\n",
    "va=VectorAssembler(inputCols=[\"Savings\",\"Income\"],\n",
    "                    outputCol=\"features\")\n",
    "processedDF=va.transform(indexedDF)\n",
    "processedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+---------+----------------+-------------+-------------+----------+\n",
      "|Savings|Income|Risk|RiskIndex|        features|rawPrediction|  probability|prediction|\n",
      "+-------+------+----+---------+----------------+-------------+-------------+----------+\n",
      "|  15000|  1100| Low|      1.0|[15000.0,1100.0]|[0.0,4.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|      0|  5000|High|      0.0|    [0.0,5000.0]|[2.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|  20000|   800|High|      0.0| [20000.0,800.0]|[3.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|   6000|  1300| Low|      1.0| [6000.0,1300.0]|[0.0,4.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|  50000|  2500| Low|      1.0|[50000.0,2500.0]|[0.0,4.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|   2000|  1100| Low|      1.0| [2000.0,1100.0]|[0.0,4.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|    700|  1500|High|      0.0|  [700.0,1500.0]|[2.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|  75000|     0|High|      0.0|   [75000.0,0.0]|[3.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|   4000|   500|High|      0.0|  [4000.0,500.0]|[3.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+-------+------+----+---------+----------------+-------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# Train a DecisionTree model\n",
    "dt = DecisionTreeClassifier(labelCol=\"RiskIndex\",\n",
    "                            featuresCol=\"features\")\n",
    "\n",
    "dtModel=dt.fit(processedDF)\n",
    "finalDF=dtModel.transform(processedDF)\n",
    "finalDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+------------------+-------------+-------------+----------+\n",
      "|Savings|Income|Risk|          features|rawPrediction|  probability|prediction|\n",
      "+-------+------+----+------------------+-------------+-------------+----------+\n",
      "| 100000| 10000| Low|[100000.0,10000.0]|[0.0,4.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|    100|   100|High|     [100.0,100.0]|[3.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|   3100|   900|High|    [3100.0,900.0]|[3.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|   2000|  1500|High|   [2000.0,1500.0]|[0.0,4.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|   3500|  1200| Low|   [3500.0,1200.0]|[0.0,4.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "+-------+------+----+------------------+-------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processedTestDF=va.transform(testData)\n",
    "finalTestDF=dtModel.transform(processedTestDF)\n",
    "finalTestDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+---------+----------------+--------------+---------------+----------+\n",
      "|Savings|Income|Risk|RiskIndex|        features| rawPrediction|    probability|prediction|\n",
      "+-------+------+----+---------+----------------+--------------+---------------+----------+\n",
      "|  15000|  1100| Low|      1.0|[15000.0,1100.0]|[2.0,18.0,0.0]|  [0.1,0.9,0.0]|       1.0|\n",
      "|      0|  5000|High|      0.0|    [0.0,5000.0]|[18.0,2.0,0.0]|  [0.9,0.1,0.0]|       0.0|\n",
      "|  20000|   800|High|      0.0| [20000.0,800.0]|[14.0,6.0,0.0]|  [0.7,0.3,0.0]|       0.0|\n",
      "|   6000|  1300| Low|      1.0| [6000.0,1300.0]|[1.0,19.0,0.0]|[0.05,0.95,0.0]|       1.0|\n",
      "|  50000|  2500| Low|      1.0|[50000.0,2500.0]|[2.0,18.0,0.0]|  [0.1,0.9,0.0]|       1.0|\n",
      "|   2000|  1100| Low|      1.0| [2000.0,1100.0]|[2.0,18.0,0.0]|  [0.1,0.9,0.0]|       1.0|\n",
      "|    700|  1500|High|      0.0|  [700.0,1500.0]|[13.0,7.0,0.0]|[0.65,0.35,0.0]|       0.0|\n",
      "|  75000|     0|High|      0.0|   [75000.0,0.0]|[17.0,3.0,0.0]|[0.85,0.15,0.0]|       0.0|\n",
      "|   4000|   500|High|      0.0|  [4000.0,500.0]|[15.0,5.0,0.0]|[0.75,0.25,0.0]|       0.0|\n",
      "+-------+------+----+---------+----------------+--------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(labelCol=\"RiskIndex\",\n",
    "                            featuresCol=\"features\",numTrees=20)\n",
    "rfModel=rf.fit(processedDF)\n",
    "finalDF=rfModel.transform(processedDF)\n",
    "finalDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+------------------+--------------+---------------+----------+\n",
      "|Savings|Income|Risk|          features| rawPrediction|    probability|prediction|\n",
      "+-------+------+----+------------------+--------------+---------------+----------+\n",
      "| 100000| 10000| Low|[100000.0,10000.0]|[5.0,15.0,0.0]|[0.25,0.75,0.0]|       1.0|\n",
      "|    100|   100|High|     [100.0,100.0]|[19.0,1.0,0.0]|[0.95,0.05,0.0]|       0.0|\n",
      "|   3100|   900|High|    [3100.0,900.0]|[15.0,5.0,0.0]|[0.75,0.25,0.0]|       0.0|\n",
      "|   2000|  1500|High|   [2000.0,1500.0]|[2.0,18.0,0.0]|  [0.1,0.9,0.0]|       1.0|\n",
      "|   3500|  1200| Low|   [3500.0,1200.0]|[4.0,16.0,0.0]|  [0.2,0.8,0.0]|       1.0|\n",
      "+-------+------+----+------------------+--------------+---------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "processedTestDF=va.transform(testData)\n",
    "finalTestDF=rfModel.transform(processedTestDF)\n",
    "finalTestDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+---------+----------------+--------------------+--------------------+----------+\n",
      "|Savings|Income|Risk|RiskIndex|        features|       rawPrediction|         probability|prediction|\n",
      "+-------+------+----+---------+----------------+--------------------+--------------------+----------+\n",
      "|  15000|  1100| Low|      1.0|[15000.0,1100.0]|[11.6679902396902...|[0.49999998466724...|       1.0|\n",
      "|      0|  5000|High|      0.0|    [0.0,5000.0]|[20.3035949416111...|[0.99999999999142...|       0.0|\n",
      "|  20000|   800|High|      0.0| [20000.0,800.0]|[11.6679902396902...|[0.49999998466724...|       1.0|\n",
      "|   6000|  1300| Low|      1.0| [6000.0,1300.0]|[11.6679902396902...|[0.49999998466724...|       1.0|\n",
      "|  50000|  2500| Low|      1.0|[50000.0,2500.0]|[11.6679902396902...|[0.49999998466724...|       1.0|\n",
      "|   2000|  1100| Low|      1.0| [2000.0,1100.0]|[3.80761564877737...|[8.57550325185138...|       1.0|\n",
      "|    700|  1500|High|      0.0|  [700.0,1500.0]|[25.7779746236880...|[0.99999999999527...|       0.0|\n",
      "|  75000|     0|High|      0.0|   [75000.0,0.0]|[11.6679902396902...|[0.49999998466724...|       1.0|\n",
      "|   4000|   500|High|      0.0|  [4000.0,500.0]|[11.6679902396902...|[0.49999998466724...|       1.0|\n",
      "+-------+------+----+---------+----------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "# specify layers for the neural network:\n",
    "# input layer of size 2 (features), one intermediate of size 4\n",
    "# and output of size 3 (classes -> Low, High, Other)\n",
    "layers = [2, 4, 3]\n",
    "# create the trainer and set its parameters\n",
    "nn = MultilayerPerceptronClassifier(labelCol=\"RiskIndex\",\n",
    "    featuresCol=\"features\",maxIter=200, layers=layers, blockSize=128,seed=1234)\n",
    "# train the model\n",
    "nnModel = nn.fit(processedDF)\n",
    "resultDF = nnModel.transform(processedDF)\n",
    "resultDF.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+---------+------------------+--------------------+--------------------+----------+\n",
      "|Savings|Income|Risk|RiskIndex|          features|       rawPrediction|         probability|prediction|\n",
      "+-------+------+----+---------+------------------+--------------------+--------------------+----------+\n",
      "| 100000| 10000| Low|      1.0|[100000.0,10000.0]|[11.6679902396902...|[0.49999998466724...|       1.0|\n",
      "|    100|   100|High|      0.0|     [100.0,100.0]|[3.80761565900288...|[8.57550343928538...|       1.0|\n",
      "|   3100|   900|High|      0.0|    [3100.0,900.0]|[11.6679902396902...|[0.49999998466724...|       1.0|\n",
      "|   2000|  1500|High|      0.0|   [2000.0,1500.0]|[3.80761564877737...|[8.57550325185138...|       1.0|\n",
      "|   3500|  1200| Low|      1.0|   [3500.0,1200.0]|[11.6679902396902...|[0.49999998466724...|       1.0|\n",
      "+-------+------+----+---------+------------------+--------------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testDataIndexed=indexerModel.transform(testData)\n",
    "processedTestDF=va.transform(testDataIndexed)\n",
    "finalTestDF=nnModel.transform(processedTestDF)\n",
    "finalTestDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Performance evalutaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training is  1.0\n",
      "Weighted precision on training is  1.0\n",
      "Accuracy on test is  0.4\n",
      "Weighted precision on test is  0.16\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "myEvaluator1 =MulticlassClassificationEvaluator(labelCol=\"RiskIndex\",predictionCol=\"prediction\",metricName='accuracy')\n",
    "myEvaluator2 =MulticlassClassificationEvaluator(labelCol=\"RiskIndex\",predictionCol=\"prediction\",metricName='weightedPrecision')\n",
    "print(\"Accuracy on training is \", myEvaluator1.evaluate(finalDF))\n",
    "print(\"Weighted precision on training is \", myEvaluator2.evaluate(finalDF))\n",
    "print(\"Accuracy on test is \", myEvaluator1.evaluate(finalTestDF))\n",
    "print(\"Weighted precision on test is \", myEvaluator2.evaluate(finalTestDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.4\n",
      "Recall = 0.4\n",
      "F1 Score = 0.4\n",
      "Class 0.0 precision = 0.0\n",
      "Class 0.0 recall = 0.0\n",
      "Class 0.0 F1 Measure = 0.0\n",
      "Class 1.0 precision = 0.4\n",
      "Class 1.0 recall = 1.0\n",
      "Class 1.0 F1 Measure = 0.5714285714285715\n",
      "Weighted recall = 0.4\n",
      "Weighted precision = 0.16\n",
      "Weighted F(1) Score = 0.2285714285714286\n",
      "Weighted F(0.5) Score = 0.18181818181818182\n",
      "Weighted false positive rate = 0.4\n"
     ]
    }
   ],
   "source": [
    "#with RDD apis\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "outRDD=finalTestDF.select(\"prediction\",\"RiskIndex\").rdd.map(lambda x: (float(x[0]),float(x[1])))\n",
    "metrics=MulticlassMetrics(outRDD)\n",
    "\n",
    "# Overall statistics\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "print(\"Summary Stats\")\n",
    "print(\"Precision = %s\" % precision)\n",
    "print(\"Recall = %s\" % recall)\n",
    "print(\"F1 Score = %s\" % f1Score)\n",
    "# Statistics by class\n",
    "labels = outRDD.map(lambda lp: lp[1]).distinct().collect()\n",
    "for label in sorted(labels):\n",
    " print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    " print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    " print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "# Weighted stats\n",
    "print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())\n",
    "print(\"Weighted F(0.5) Score = %s\" % metrics.weightedFMeasure(beta=0.5))\n",
    "print(\"Weighted false positive rate = %s\" % metrics.weightedFalsePositiveRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+---------+----------------+-------------+-------------+----------+\n",
      "|Savings|Income|Risk|RiskIndex|        features|rawPrediction|  probability|prediction|\n",
      "+-------+------+----+---------+----------------+-------------+-------------+----------+\n",
      "|  15000|  1100| Low|      1.0|[15000.0,1100.0]|[0.0,4.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|      0|  5000|High|      0.0|    [0.0,5000.0]|[2.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|  20000|   800|High|      0.0| [20000.0,800.0]|[3.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|   6000|  1300| Low|      1.0| [6000.0,1300.0]|[0.0,4.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|  50000|  2500| Low|      1.0|[50000.0,2500.0]|[0.0,4.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|   2000|  1100| Low|      1.0| [2000.0,1100.0]|[0.0,4.0,0.0]|[0.0,1.0,0.0]|       1.0|\n",
      "|    700|  1500|High|      0.0|  [700.0,1500.0]|[2.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|  75000|     0|High|      0.0|   [75000.0,0.0]|[3.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|   4000|   500|High|      0.0|  [4000.0,500.0]|[3.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+-------+------+----+---------+----------------+-------------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"RiskIndex\", featuresCol=\"features\")\n",
    "paramGrid = ParamGridBuilder()\\\n",
    " .addGrid(dt.maxDepth, [1,2,10]) \\\n",
    " .addGrid(dt.impurity, [\"Gini\",\"Entropy\"])\\\n",
    " .build()\n",
    "myEvaluator =MulticlassClassificationEvaluator(labelCol=\"RiskIndex\",predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "cv=CrossValidator(estimator=dt,evaluator=myEvaluator,estimatorParamMaps=paramGrid, numFolds=3)\n",
    "cvModel=cv.fit(processedDF)\n",
    "finalDF=cvModel.transform(processedDF)\n",
    "finalDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='DecisionTreeClassifier_5f0f20325631', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 2,\n",
       " Param(parent='DecisionTreeClassifier_5f0f20325631', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'Gini'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "cvModel.getEstimatorParamMaps()[numpy.argmax(cvModel.avgMetrics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
