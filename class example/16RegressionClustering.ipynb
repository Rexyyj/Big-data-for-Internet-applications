{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rentalsDF = spark.createDataFrame([\n",
    "    (\"Monday\",1.5,358),\n",
    "    (\"Saturday\",1.0,272),\n",
    "    (\"Saturday\",0.5,390),\n",
    "    (\"Monday\",3.0,120),\n",
    "    (\"Saturday\",0.3,439),\n",
    "    (\"Monday\",0.9,509),\n",
    "    (\"Saturday\",1.9,102),\n",
    "    (\"Saturday\",2.7,43),\n",
    "    (\"Monday\",0.6,597),\n",
    "],[\"weekDay\",\"distanceCenter\",\"rentals\"])\n",
    "rentalsTestDF = spark.createDataFrame([\n",
    "    (\"Monday\",0.1,641),\n",
    "    (\"Saturday\",2.1,129),\n",
    "    (\"Saturday\",1.5,199),\n",
    "    (\"Monday\",2.0,231),\n",
    "    (\"Sunday\",0.5,393)\n",
    "],[\"weekDay\",\"distanceCenter\",\"rentals\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"weekDay\",\n",
    "            outputCol=\"weekDayIndex\", handleInvalid=\"keep\")\n",
    "indexerModel = indexer.fit(rentalsDF)\n",
    "indexedDF=indexerModel.transform(rentalsDF)\n",
    "\n",
    "va=VectorAssembler(inputCols=[\"weekDayIndex\",\"distanceCenter\"],\n",
    "outputCol=\"features\")\n",
    "assembledDF=va.transform(indexedDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-------+------------+---------+------------------+\n",
      "| weekDay|distanceCenter|rentals|weekDayIndex| features|        prediction|\n",
      "+--------+--------------+-------+------------+---------+------------------+\n",
      "|  Monday|           1.5|    358|         1.0|[1.0,1.5]| 396.0000000000001|\n",
      "|Saturday|           1.0|    272|         0.0|[0.0,1.0]| 299.7777182645956|\n",
      "|Saturday|           0.5|    390|         0.0|[0.0,0.5]|390.09507230851636|\n",
      "|  Monday|           3.0|    120|         1.0|[1.0,3.0]| 125.0479378682378|\n",
      "|Saturday|           0.3|    439|         0.0|[0.0,0.3]| 426.2220139260847|\n",
      "|  Monday|           0.9|    509|         1.0|[1.0,0.9]|  504.380824852705|\n",
      "|Saturday|           1.9|    102|         0.0|[0.0,1.9]|137.20648098553824|\n",
      "|Saturday|           2.7|     43|         0.0|[0.0,2.7]|-7.301285484735047|\n",
      "|  Monday|           0.6|    597|         1.0|[1.0,0.6]| 558.5712372790575|\n",
      "+--------+--------------+-------+------------+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(labelCol=\"rentals\",featuresCol=\"features\",maxIter=10)\n",
    "# Fit the model\n",
    "lrModel = lr.fit(assembledDF)\n",
    "# Create the predictions\n",
    "predictionDF=lrModel.transform(assembledDF)\n",
    "predictionDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [186.53963577932527,-180.63470808784155]\n",
      "Intercept: 480.41242635243714\n",
      "RMSE: 29.197016\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-38.000000000000114|\n",
      "|-27.777718264595592|\n",
      "|-0.0950723085163645|\n",
      "|-5.0479378682377956|\n",
      "| 12.777986073915315|\n",
      "|  4.619175147294982|\n",
      "|-35.206480985538235|\n",
      "|  50.30128548473505|\n",
      "|  38.42876272094247|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "trainingSummary.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-------+------------+---------+------------------+\n",
      "| weekDay|distanceCenter|rentals|weekDayIndex| features|        prediction|\n",
      "+--------+--------------+-------+------------+---------+------------------+\n",
      "|  Monday|           0.1|    641|         1.0|[1.0,0.1]| 648.8885913229783|\n",
      "|Saturday|           2.1|    129|         0.0|[0.0,2.1]|101.07953936796986|\n",
      "|Saturday|           1.5|    199|         0.0|[0.0,1.5]|209.46036422067482|\n",
      "|  Monday|           2.0|    231|         1.0|[1.0,2.0]|305.68264595607934|\n",
      "|  Sunday|           0.5|    393|         2.0|[2.0,0.5]|  763.174343867167|\n",
      "+--------+--------------+-------+------------+---------+------------------+\n",
      "\n",
      "Root Mean Squared Error (RMSE) on test data = 169.445\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "indexedTestDF=indexerModel.transform(rentalsTestDF)\n",
    "assembledTestDF=va.transform(indexedTestDF)\n",
    "predictionTestDF=lrModel.transform(assembledTestDF)\n",
    "predictionTestDF.show()\n",
    "# compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    " labelCol=\"rentals\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictionTestDF)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+-------+------------+---------+----------+\n",
      "| weekDay|distanceCenter|rentals|weekDayIndex| features|prediction|\n",
      "+--------+--------------+-------+------------+---------+----------+\n",
      "|  Monday|           1.5|    358|         1.0|[1.0,1.5]|     358.0|\n",
      "|Saturday|           1.0|    272|         0.0|[0.0,1.0]|     272.0|\n",
      "|Saturday|           0.5|    390|         0.0|[0.0,0.5]|     390.0|\n",
      "|  Monday|           3.0|    120|         1.0|[1.0,3.0]|     120.0|\n",
      "|Saturday|           0.3|    439|         0.0|[0.0,0.3]|     439.0|\n",
      "|  Monday|           0.9|    509|         1.0|[1.0,0.9]|     509.0|\n",
      "|Saturday|           1.9|    102|         0.0|[0.0,1.9]|     102.0|\n",
      "|Saturday|           2.7|     43|         0.0|[0.0,2.7]|      43.0|\n",
      "|  Monday|           0.6|    597|         1.0|[1.0,0.6]|     597.0|\n",
      "+--------+--------------+-------+------------+---------+----------+\n",
      "\n",
      "Root Mean Squared Error (RMSE) on training data = 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeRegressor(labelCol=\"rentals\",featuresCol=\"features\",maxDepth=4)\n",
    "# Fit the model\n",
    "dtModel = dt.fit(assembledDF)\n",
    "# Predict output\n",
    "predictionDF=dtModel.transform(assembledDF)\n",
    "predictionDF.show()\n",
    "# Compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    " labelCol=\"rentals\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictionDF)\n",
    "print(\"Root Mean Squared Error (RMSE) on training data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on training data = 169.445\n"
     ]
    }
   ],
   "source": [
    "predictionTestDF=lrModel.transform(assembledTestDF)\n",
    "# compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    " labelCol=\"rentals\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictionTestDF)\n",
    "print(\"Root Mean Squared Error (RMSE) on training data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Unsupervised learning: clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.createDataFrame([\n",
    "    (15000,1000,\"Paolo\"),\n",
    "    (0,5000,\"Luca\"),\n",
    "    (20000,800,\"Martino\"),\n",
    "    (6000,1300,\"Mike\"),\n",
    "    (50000,2500,\"Francesca\"),\n",
    "    (2000,1100,\"Steve\"),\n",
    "    (700,1500,\"Maria\"),\n",
    "    (75000,0,\"Guido\"),\n",
    "    (4000,500,\"Roberta\"),\n",
    "    (7000,3000,\"Idilio\"),\n",
    "    (3000,900,\"Marco\"),\n",
    "    (6000,1200,\"Dena\"),\n",
    "],[\"Savings\",\"Income\",\"User\"])\n",
    "dataNewDF = spark.createDataFrame([\n",
    "    (10000,1860,\"MARIANA\"),\n",
    "    (4500,1100,\"Nicola\"),\n",
    "    (27000,1000,\"Davide\"),\n",
    "],[\"Savings\",\"Income\",\"User\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+----------------+--------------------+\n",
      "|Savings|Income|     User|        features|      scaledFeatures|\n",
      "+-------+------+---------+----------------+--------------------+\n",
      "|  15000|  1000|    Paolo|[15000.0,1000.0]|[-0.0312169519277...|\n",
      "|      0|  5000|     Luca|    [0.0,5000.0]|[-0.6770849228476...|\n",
      "|  20000|   800|  Martino| [20000.0,800.0]|[0.18407237171215...|\n",
      "|   6000|  1300|     Mike| [6000.0,1300.0]|[-0.4187377344796...|\n",
      "|  50000|  2500|Francesca|[50000.0,2500.0]|[1.47580831355180...|\n",
      "|   2000|  1100|    Steve| [2000.0,1100.0]|[-0.5909691933916...|\n",
      "|    700|  1500|    Maria|  [700.0,1500.0]|[-0.6469444175380...|\n",
      "|  75000|     0|    Guido|   [75000.0,0.0]|[2.55225493175152...|\n",
      "|   4000|   500|  Roberta|  [4000.0,500.0]|[-0.5048534639356...|\n",
      "|   7000|  3000|   Idilio| [7000.0,3000.0]|[-0.3756798697517...|\n",
      "|   3000|   900|    Marco|  [3000.0,900.0]|[-0.5479113286636...|\n",
      "|   6000|  1200|     Dena| [6000.0,1200.0]|[-0.4187377344796...|\n",
      "+-------+------+---------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va=VectorAssembler(inputCols=[\"Savings\",\"Income\"],\n",
    "                    outputCol=\"features\")\n",
    "assembledDF=va.transform(data)\n",
    "scaler = StandardScaler(inputCol=\"features\",\n",
    "                    outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(assembledDF)\n",
    "scaledDF=scalerModel.transform(assembledDF)\n",
    "scaledDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* K-means clustering algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+----------------+--------------------+----------+\n",
      "|Savings|Income|     User|        features|      scaledFeatures|prediction|\n",
      "+-------+------+---------+----------------+--------------------+----------+\n",
      "|  15000|  1000|    Paolo|[15000.0,1000.0]|[-0.0312169519277...|         0|\n",
      "|      0|  5000|     Luca|    [0.0,5000.0]|[-0.6770849228476...|         1|\n",
      "|  20000|   800|  Martino| [20000.0,800.0]|[0.18407237171215...|         0|\n",
      "|   6000|  1300|     Mike| [6000.0,1300.0]|[-0.4187377344796...|         0|\n",
      "|  50000|  2500|Francesca|[50000.0,2500.0]|[1.47580831355180...|         2|\n",
      "|   2000|  1100|    Steve| [2000.0,1100.0]|[-0.5909691933916...|         0|\n",
      "|    700|  1500|    Maria|  [700.0,1500.0]|[-0.6469444175380...|         0|\n",
      "|  75000|     0|    Guido|   [75000.0,0.0]|[2.55225493175152...|         2|\n",
      "|   4000|   500|  Roberta|  [4000.0,500.0]|[-0.5048534639356...|         0|\n",
      "|   7000|  3000|   Idilio| [7000.0,3000.0]|[-0.3756798697517...|         1|\n",
      "|   3000|   900|    Marco|  [3000.0,900.0]|[-0.5479113286636...|         0|\n",
      "|   6000|  1200|     Dena| [6000.0,1200.0]|[-0.4187377344796...|         0|\n",
      "+-------+------+---------+----------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "# Trains a k-means model.\n",
    "kmeans = KMeans(k=3,featuresCol=\"scaledFeatures\",initMode=\"k-means||\")\n",
    "model = kmeans.fit(scaledDF)\n",
    "# Make predictions\n",
    "predictionsDF = model.transform(scaledDF)\n",
    "predictionsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[-0.37191231 -0.39159297]\n",
      "[-0.5263824   1.80071098]\n",
      "[ 2.01403162 -0.2343391 ]\n",
      "Size of the clusters:  [8, 2, 2]\n",
      "Silhouette with squared euclidean distance = -0.09641007707651222\n",
      "SSE:  4.404936191705297\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "print(\"Size of the clusters: \", model.summary.clusterSizes)\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictionsDF)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "print(\"SSE: \",model.computeCost(predictionsDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+----------------+--------------------+----------+\n",
      "|Savings|Income|   User|        features|      scaledFeatures|prediction|\n",
      "+-------+------+-------+----------------+--------------------+----------+\n",
      "|  10000|  1860|MARIANA|[10000.0,1860.0]|[-0.2465062755677...|         0|\n",
      "|   4500|  1100| Nicola| [4500.0,1100.0]|[-0.4833245315716...|         0|\n",
      "|  27000|  1000| Davide|[27000.0,1000.0]|[0.48547742480807...|         0|\n",
      "+-------+------+-------+----------------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembledNewDF=va.transform(dataNewDF)\n",
    "scaledNewDF=scalerModel.transform(assembledNewDF)\n",
    "# Make predictions\n",
    "predictionsNewDF = model.transform(scaledNewDF)\n",
    "predictionsNewDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gaussian mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------+----------------+-------------------------------------------+----------+------------------------------------------------------------------+\n",
      "|Savings|Income|User     |features        |scaledFeatures                             |prediction|probability                                                       |\n",
      "+-------+------+---------+----------------+-------------------------------------------+----------+------------------------------------------------------------------+\n",
      "|15000  |1000  |Paolo    |[15000.0,1000.0]|[-0.031216951927791736,-0.4193436518555962]|1         |[2.64050592481616E-16,0.9999999999999994,2.64050592481616E-16]    |\n",
      "|0      |5000  |Luca     |[0.0,5000.0]    |[-0.6770849228476208,2.5407291847721423]   |2         |[2.473983276643728E-21,2.473983276643728E-21,1.0]                 |\n",
      "|20000  |800   |Martino  |[20000.0,800.0] |[0.18407237171215127,-0.5673472936869831]  |1         |[9.232934078192929E-16,0.9999999999999981,9.232934078192929E-16]  |\n",
      "|6000   |1300  |Mike     |[6000.0,1300.0] |[-0.41873773447968915,-0.1973381891085158] |1         |[1.7694918388522463E-16,0.9999999999999997,1.7694918388522463E-16]|\n",
      "|50000  |2500  |Francesca|[50000.0,2500.0]|[1.4758083135518092,0.6906836618798058]    |0         |[1.0,8.587081373350302E-22,8.587079493451898E-22]                 |\n",
      "|2000   |1100  |Steve    |[2000.0,1100.0] |[-0.5909691933916436,-0.3453418309399027]  |1         |[1.6096419640014307E-16,0.9999999999999997,1.6096419640014307E-16]|\n",
      "|700    |1500  |Maria    |[700.0,1500.0]  |[-0.6469444175380287,-0.04933454727712887] |1         |[4.693328952874187E-16,0.9999999999999991,4.693328952874187E-16]  |\n",
      "|75000  |0     |Guido    |[75000.0,0.0]   |[2.5522549317515244,-1.159361861012531]    |2         |[2.473983276643728E-21,2.4739832810513254E-21,1.0]                |\n",
      "|4000   |500   |Roberta  |[4000.0,500.0]  |[-0.5048534639356663,-0.7893527564340636]  |1         |[1.2495444751275418E-15,0.9999999999999974,1.2495444751275418E-15]|\n",
      "|7000   |3000  |Idilio   |[7000.0,3000.0] |[-0.37567986975170053,1.060692766458273]   |0         |[0.9999999999999999,6.781606129091991E-17,8.587079493451898E-22]  |\n",
      "|3000   |900   |Marco    |[3000.0,900.0]  |[-0.547911328663655,-0.49334547277128965]  |1         |[1.8645493704109402E-16,0.9999999999999997,1.8645493704109402E-16]|\n",
      "|6000   |1200  |Dena     |[6000.0,1200.0] |[-0.41873773447968915,-0.27134001002420927]|1         |[1.365494944563097E-16,0.9999999999999998,1.365494944563097E-16]  |\n",
      "+-------+------+---------+----------------+-------------------------------------------+----------+------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import GaussianMixture\n",
    "# Trains a GMM model.\n",
    "gmm = GaussianMixture(k=3,featuresCol=\"scaledFeatures\")\n",
    "model = gmm.fit(scaledDF)\n",
    "# Make predictions\n",
    "predictionsDF = model.transform(scaledDF)\n",
    "predictionsDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussians weights shown as a DataFrame: \n",
      "+-----------------------------------------+---------------------------------------------------------------------------------------------+\n",
      "|mean                                     |cov                                                                                          |\n",
      "+-----------------------------------------+---------------------------------------------------------------------------------------------+\n",
      "|[0.5500642219000528,0.8756882141690369]  |0.8570021232281327    -0.17126687121043363  \n",
      "-0.17126687121043363  0.03422668436774328       |\n",
      "|[-0.37191230658800156,-0.391592969012211]|0.07523188967303805    -0.019421862342788105  \n",
      "-0.019421862342788105  0.046462724029206745   |\n",
      "|[0.9375850044519495,0.6906836618798035]  |2.6071589741256043  -2.98721286994946  \n",
      "-2.98721286994946   3.422668436773975                |\n",
      "+-----------------------------------------+---------------------------------------------------------------------------------------------+\n",
      "\n",
      "Size of the clusters:  [2, 8, 2]\n",
      "Silhouette with squared euclidean distance = 0.36520049677492744\n"
     ]
    }
   ],
   "source": [
    "print(\"Gaussians weights shown as a DataFrame: \")\n",
    "model.gaussiansDF.show(truncate=False)\n",
    "print(\"Size of the clusters: \", model.summary.clusterSizes)\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictionsDF)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-------+----------------+--------------------+----------+--------------------+\n",
      "|Savings|Income|   User|        features|      scaledFeatures|prediction|         probability|\n",
      "+-------+------+-------+----------------+--------------------+----------+--------------------+\n",
      "|  10000|  1860|MARIANA|[10000.0,1860.0]|[-0.2465062755677...|         1|[1.84461323121284...|\n",
      "|   4500|  1100| Nicola| [4500.0,1100.0]|[-0.4833245315716...|         1|[1.27379113530588...|\n",
      "|  27000|  1000| Davide|[27000.0,1000.0]|[0.48547742480807...|         1|[2.43128540013928...|\n",
      "+-------+------+-------+----------------+--------------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembledNewDF=va.transform(dataNewDF)\n",
    "scaledNewDF=scalerModel.transform(assembledNewDF)\n",
    "# Make predictions\n",
    "predictionsNewDF = model.transform(scaledNewDF)\n",
    "predictionsNewDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
