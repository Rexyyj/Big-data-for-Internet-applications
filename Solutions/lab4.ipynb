{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of columns in the file =  207\n",
      "Number of connection log =  100000\n"
     ]
    }
   ],
   "source": [
    "inputDF = spark.read.load(\"/data/students/bigdata_internet/lab4/log_tcp_complete_classes.txt\",\n",
    "                        format=\"csv\",\n",
    "                        header=True,\n",
    "                        inferSchema=True,\n",
    "                        sep=' ')\n",
    "\n",
    "print(\"number of columns in the file = \",len(inputDF.columns))\n",
    "print(\"Number of connection log = \" , inputDF.count())\n",
    "# inputDF.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify TCP Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes =  10\n",
      "+---------------+-----+\n",
      "|      class:207|count|\n",
      "+---------------+-----+\n",
      "|   class:google|10000|\n",
      "|   class:amazon|10000|\n",
      "|class:instagram|10000|\n",
      "| class:facebook|10000|\n",
      "|  class:netflix|10000|\n",
      "|     class:ebay|10000|\n",
      "|  class:spotify|10000|\n",
      "| class:linkedin|10000|\n",
      "|  class:youtube|10000|\n",
      "|     class:bing|10000|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classesDF = inputDF.select(\"class:207\")\n",
    "print(\"Number of classes = \",classesDF.distinct().count())\n",
    "classesDF.groupBy(\"class:207\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size:  75072\n",
      "Testing set size:  24928\n"
     ]
    }
   ],
   "source": [
    "trainDF,testDF = inputDF.randomSplit([0.75,0.25])\n",
    "print(\"Training set size: \",trainDF.count())\n",
    "print(\"Testing set size: \",testDF.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator\n",
    "# transform the labels to indexs\n",
    "labelIndexer = StringIndexer(inputCol=\"class:207\", outputCol=\"service\",handleInvalid=\"keep\")\n",
    "labelIndexerModel = labelIndexer.fit(trainDF)\n",
    "processedTrainDF=labelIndexerModel.transform(trainDF)\n",
    "processedTestDF = labelIndexerModel.transform(testDF)\n",
    "\n",
    "#trans form categorical features to indexs\n",
    "feature1Indexer = StringIndexer(inputCol=\"con_t:42\", outputCol=\"connectionType\",handleInvalid=\"keep\")\n",
    "feature1IndexerModel = feature1Indexer.fit(processedTrainDF)\n",
    "processedTrainDF=feature1IndexerModel.transform(processedTrainDF)\n",
    "processedTestDF=feature1IndexerModel.transform(processedTestDF)\n",
    "feature2Indexer = StringIndexer(inputCol=\"http_t:44\", outputCol=\"httpType\",handleInvalid=\"keep\")\n",
    "feature2IndexerModel = feature2Indexer.fit(processedTrainDF)\n",
    "processedTrainDF=feature2IndexerModel.transform(processedTrainDF)\n",
    "processedTestDF=feature2IndexerModel.transform(processedTestDF)\n",
    "\n",
    "va=VectorAssembler(inputCols=[\"s_bytes_uniq:21\",\"durat:31\",\"connectionType\",\"httpType\",\"c_rtt_avg:45\",\"s_rtt_avg:52\",\"s_pkts_data_avg:197\",\n",
    "                             \"c_pkts_push:114\",\"s_pkts_push:115\",\"c_msgsize_count:131\",\"s_msgsize_count:142\",\"c_pkts_data_std:195\",\n",
    "                             \"s_pkts_data_std:198\",\"c_sit_std:201\",\"s_sit_std:204\"],\n",
    "                    outputCol=\"features\")\n",
    "\n",
    "processedTrainDF=va.transform(processedTrainDF)\n",
    "processedTestDF=va.transform(processedTestDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train at least two different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest training time(s):  4.722044946043752\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "import timeit\n",
    "rf = RandomForestClassifier(labelCol=\"service\",\n",
    "                            featuresCol=\"features\",numTrees=20)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "rfModel=rf.fit(processedTrainDF)\n",
    "stop = timeit.default_timer()\n",
    "print('Random forest training time(s): ', stop - start)  \n",
    "\n",
    "rfFinalTrainDF=rfModel.transform(processedTrainDF)\n",
    "rfFinalTestDF=rfModel.transform(processedTestDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest training time(s):  2.5945384330116212\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "# Train a DecisionTree model\n",
    "dt = DecisionTreeClassifier(labelCol=\"service\",featuresCol=\"features\")\n",
    "start = timeit.default_timer()\n",
    "dtModel=dt.fit(processedTrainDF)\n",
    "stop = timeit.default_timer()\n",
    "print('Random forest training time(s): ', stop - start)  \n",
    "\n",
    "dtFinalTrainDF=dtModel.transform(processedTrainDF)\n",
    "dtFinalTestDF=dtModel.transform(processedTestDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of random forest:\n",
      "Accuracy on training is  0.6558637041773231\n",
      "F1 measure on training is  0.6541723859860844\n",
      "Accuracy on test is  0.6573732349165597\n",
      "F1 measure on test is  0.6557234069448274\n",
      "Evaluation of decision tree\n",
      "Accuracy on training is  0.558490515771526\n",
      "F1 measure on training is  0.5540048518808488\n",
      "Accuracy on test is  0.5568838254172015\n",
      "F1 measure on test is  0.5528345011507971\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "myEvaluator1 =MulticlassClassificationEvaluator(labelCol=\"service\",predictionCol=\"prediction\",metricName='accuracy')\n",
    "myEvaluator2 =MulticlassClassificationEvaluator(labelCol=\"service\",predictionCol=\"prediction\",metricName='f1')\n",
    "print(\"Evalutation of random forest:\")\n",
    "print(\"Accuracy on training is \", myEvaluator1.evaluate(rfFinalTrainDF))\n",
    "print(\"F1 measure on training is \", myEvaluator2.evaluate(rfFinalTrainDF))\n",
    "print(\"Accuracy on test is \", myEvaluator1.evaluate(rfFinalTestDF))\n",
    "print(\"F1 measure on test is \", myEvaluator2.evaluate(rfFinalTestDF))\n",
    "print(\"Evaluation of decision tree\")\n",
    "print(\"Accuracy on training is \", myEvaluator1.evaluate(dtFinalTrainDF))\n",
    "print(\"F1 measure on training is \", myEvaluator2.evaluate(dtFinalTrainDF))\n",
    "print(\"Accuracy on test is \", myEvaluator1.evaluate(dtFinalTestDF))\n",
    "print(\"F1 measure on test is \", myEvaluator2.evaluate(dtFinalTestDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune the parameters of the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_6796dcbc3599', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 20,\n",
       " Param(parent='RandomForestClassifier_6796dcbc3599', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'Entropy',\n",
       " Param(parent='RandomForestClassifier_6796dcbc3599', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 5,\n",
       " Param(parent='RandomForestClassifier_6796dcbc3599', name='numTrees', doc='Number of trees to train (>= 1).'): 30}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "import numpy\n",
    "rf = RandomForestClassifier(labelCol=\"service\",featuresCol=\"features\")\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(rf.maxDepth, [10,2,20]) \\\n",
    "    .addGrid(rf.impurity, [\"Gini\",\"Entropy\"])\\\n",
    "    .addGrid(rf.minInstancesPerNode,[5,5,30])\\\n",
    "    .addGrid(rf.numTrees, [1,10,30])\\\n",
    "    .build()\n",
    "myEvaluator =MulticlassClassificationEvaluator(labelCol=\"service\",predictionCol=\"prediction\",\n",
    "                                               metricName=\"accuracy\")\n",
    "cv=CrossValidator(estimator=rf,evaluator=myEvaluator,estimatorParamMaps=paramGrid, numFolds=3)\n",
    "cvModel=cv.fit(processedTrainDF)\n",
    "rfFinalTrainDF=cvModel.transform(processedTrainDF)\n",
    "rfFinalTestDF=cvModel.transform(processedTestDF)\n",
    "cvModel.getEstimatorParamMaps()[numpy.argmax(cvModel.avgMetrics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning of decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='RandomForestClassifier_cd916a51ea2f', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.'): 10,\n",
       " Param(parent='RandomForestClassifier_cd916a51ea2f', name='impurity', doc='Criterion used for information gain calculation (case-insensitive). Supported options: entropy, gini'): 'Gini',\n",
       " Param(parent='RandomForestClassifier_cd916a51ea2f', name='minInstancesPerNode', doc='Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1.'): 5}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "import numpy\n",
    "dt = DecisionTreeClassifier(labelCol=\"service\",featuresCol=\"features\")\n",
    "paramGriddt = ParamGridBuilder()\\\n",
    "    .addGrid(rf.maxDepth, [10,15,20]) \\\n",
    "    .addGrid(rf.impurity, [\"Gini\",\"Entropy\"])\\\n",
    "    .addGrid(rf.minInstancesPerNode,[5,8,10])\\\n",
    "    .build()\n",
    "myEvaluator =MulticlassClassificationEvaluator(labelCol=\"service\",predictionCol=\"prediction\",\n",
    "                                               metricName=\"accuracy\")\n",
    "cv2=CrossValidator(estimator=dt,evaluator=myEvaluator,estimatorParamMaps=paramGriddt, numFolds=3)\n",
    "cv2Model=cv2.fit(processedTrainDF)\n",
    "dtFinalTrainDF=cv2Model.transform(processedTrainDF)\n",
    "dtFinalTestDF=cv2Model.transform(processedTestDF)\n",
    "cv2Model.getEstimatorParamMaps()[numpy.argmax(cv2Model.avgMetrics)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return the best possible model and estimate its performance on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of the besst possible model of random forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of random forest:\n",
      "Accuracy on training is  0.9499712708954127\n",
      "F1 measure on training is  0.9499347149912925\n",
      "Accuracy on test is  0.9126097842069706\n",
      "F1 measure on test is  0.9125492365870005\n"
     ]
    }
   ],
   "source": [
    "rfOptimal = RandomForestClassifier(labelCol=\"service\",featuresCol=\"features\",numTrees=30,maxDepth=20,\n",
    "                                   impurity=\"Entropy\",minInstancesPerNode=5)\n",
    "rfOptimalModel =rfOptimal.fit(processedTrainDF)\n",
    "rfFinalTrainDF=rfOptimalModel.transform(processedTrainDF)\n",
    "rfFinalTestDF=rfOptimalModel.transform(processedTestDF)\n",
    "print(\"Evalutation of random forest:\")\n",
    "print(\"Accuracy on training is \", myEvaluator1.evaluate(rfFinalTrainDF))\n",
    "print(\"F1 measure on training is \", myEvaluator2.evaluate(rfFinalTrainDF))\n",
    "print(\"Accuracy on test is \", myEvaluator1.evaluate(rfFinalTestDF))\n",
    "print(\"F1 measure on test is \", myEvaluator2.evaluate(rfFinalTestDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of the besst possible model of decision tree classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evalutation of neural network:\n",
      "Accuracy on training is  0.8009111253196931\n",
      "F1 measure on training is  0.8026785345361586\n",
      "Accuracy on test is  0.7901957637997432\n",
      "F1 measure on test is  0.7922922855780302\n"
     ]
    }
   ],
   "source": [
    "dtOptimal = DecisionTreeClassifier(labelCol=\"service\",featuresCol=\"features\",maxDepth=10,\n",
    "                                   impurity=\"Gini\",minInstancesPerNode=5)\n",
    "dtOptimalModel =dtOptimal.fit(processedTrainDF)\n",
    "dtFinalTrainDF=dtOptimalModel.transform(processedTrainDF)\n",
    "dtFinalTestDF=dtOptimalModel.transform(processedTestDF)\n",
    "print(\"Evalutation of neural network:\")\n",
    "print(\"Accuracy on training is \", myEvaluator1.evaluate(dtFinalTrainDF))\n",
    "print(\"F1 measure on training is \", myEvaluator2.evaluate(dtFinalTrainDF))\n",
    "print(\"Accuracy on test is \", myEvaluator1.evaluate(dtFinalTestDF))\n",
    "print(\"F1 measure on test is \", myEvaluator2.evaluate(dtFinalTestDF))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster users(Bonus Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer to question: \n",
      "Total number of clients in the file is:  3844\n",
      "Average number of connections:  26.014568158168576\n"
     ]
    }
   ],
   "source": [
    "ipAllDF = inputDF.select(\"#31#c_ip:1\")\n",
    "clientNum = ipAllDF.distinct().count()\n",
    "print(\"Answer to question: \")\n",
    "print(\"Total number of clients in the file is: \",clientNum)\n",
    "print(\"Average number of connections: \",ipAllDF.count()/clientNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----------------+----------------+-----------------+------------------+------------------+\n",
      "|            key|count|sum(c_bytes_all)|sum(s_bytes_all)|sum(s_bytes_retx)|    avg(s_rtt_avg)|      avg(s_first)|\n",
      "+---------------+-----+----------------+----------------+-----------------+------------------+------------------+\n",
      "|  156.60.18.189|   95|           90794|          561767|            18017| 68.93080024210525|111.40607368421053|\n",
      "|   246.25.87.44|    1|            1996|            6395|                0|         29.023549|            47.214|\n",
      "|254.222.227.249|   20|          100315|           93481|                5| 31.46155785000001|         727.81405|\n",
      "|180.102.208.155|   36|          435819|        98356251|           468531|20.410008222222224| 135.3493611111111|\n",
      "|  180.102.5.237|   81|          136791|         1153915|            66095| 75.14003027160494|246.04407407407408|\n",
      "+---------------+-----+----------------+----------------+-----------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# compute statics of features\n",
    "TCPNumDF = inputDF.selectExpr(\"`#31#c_ip:1` AS key\").groupBy(\"key\").count()\n",
    "sumBytesDF = inputDF.selectExpr(\"`#31#c_ip:1` AS key\" ,\"`c_bytes_all:9`AS c_bytes_all\",\"`s_bytes_all:23` AS s_bytes_all\",\"`s_bytes_retx:25` AS s_bytes_retx\")\\\n",
    "            .groupBy(\"key\").sum(\"c_bytes_all\",\"s_bytes_all\",\"s_bytes_retx\")\n",
    "avgBytesDF = inputDF.selectExpr(\"`#31#c_ip:1` AS key\",\"`s_rtt_avg:52` AS s_rtt_avg\",\"`s_first:33` AS s_first\").groupBy(\"key\")\\\n",
    "                .avg(\"s_rtt_avg\",\"s_first\") \n",
    "featuresByIPDF = TCPNumDF.join(sumBytesDF,\"key\").join(avgBytesDF,\"key\")     \n",
    "featuresByIPDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+----------------+----------------+-----------------+------------------+------------------+--------------------+--------------------+\n",
      "|            key|count|sum(c_bytes_all)|sum(s_bytes_all)|sum(s_bytes_retx)|    avg(s_rtt_avg)|      avg(s_first)|            features|      scaledFeatures|\n",
      "+---------------+-----+----------------+----------------+-----------------+------------------+------------------+--------------------+--------------------+\n",
      "|  156.60.18.189|   95|           90794|          561767|            18017| 68.93080024210525|111.40607368421053|[95.0,90794.0,561...|[1.49221510349479...|\n",
      "|   246.25.87.44|    1|            1996|            6395|                0|         29.023549|            47.214|[1.0,1996.0,6395....|[-0.5410869427997...|\n",
      "|254.222.227.249|   20|          100315|           93481|                5| 31.46155785000001|         727.81405|[20.0,100315.0,93...|[-0.1301003589742...|\n",
      "|180.102.208.155|   36|          435819|        98356251|           468531|20.410008222222224| 135.3493611111111|[36.0,435819.0,9....|[0.21599360635245...|\n",
      "|  180.102.5.237|   81|          136791|         1153915|            66095| 75.14003027160494|246.04407407407408|[81.0,136791.0,11...|[1.18938288383389...|\n",
      "+---------------+-----+----------------+----------------+-----------------+------------------+------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prerpocess the data\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "va=VectorAssembler(inputCols=[\"count\",\"sum(c_bytes_all)\",\"sum(s_bytes_all)\",\"sum(s_bytes_retx)\",\n",
    "                              \"avg(s_rtt_avg)\",\"avg(s_first)\"],\n",
    "                    outputCol=\"features\")\n",
    "assembledDF=va.transform(featuresByIPDF)\n",
    "scaler = StandardScaler(inputCol=\"features\",\n",
    "                    outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(assembledDF)\n",
    "scaledDF=scalerModel.transform(assembledDF)\n",
    "scaledDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[-0.30493536 -0.03539213 -0.13581139 -0.12737719 -0.09785649 -0.13895952]\n",
      "[ 5.92068567  0.17778297  0.1846758   0.30958307 -0.02271687  0.16099715]\n",
      "[-0.0840172  -0.03619209 -0.14202217 -0.07738945  0.15869617  6.325802  ]\n",
      "[-0.2598856  -0.04104792 -0.15716115 -0.15292926 37.53320366  5.64464997]\n",
      "[ 1.27590638  0.096142   21.75941001  8.81227512 -0.17932665 -0.20520205]\n",
      "[ 1.55092747  0.1442749   8.76731777 18.71750646  0.06336384  0.18616852]\n",
      "[ 0.08491052  0.13849213 -0.09556905  0.01399882  4.74066284  1.02361115]\n",
      "[ 1.08973903  0.0581222   4.23515868  2.73132747  0.03676071 -0.05599182]\n",
      "[1.09326835 0.17180427 0.07242403 0.11079759 0.03765219 0.09164826]\n",
      "[-0.51945607 -0.04294102 -0.16679048 -0.15299766  0.18195395 28.9720216 ]\n",
      "Size of the clusters:  [3101, 48, 46, 1, 3, 7, 47, 61, 529, 1]\n",
      "Silhouette with squared euclidean distance = -0.9327856315897602\n",
      "SSE:  8621.659083725388\n"
     ]
    }
   ],
   "source": [
    "# Cluster with K-means and Evaluate\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "kmeans = KMeans(k=10,featuresCol=\"scaledFeatures\",initMode=\"k-means||\")\n",
    "model = kmeans.fit(scaledDF)\n",
    "predictionsDF = model.transform(scaledDF)\n",
    "\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)\n",
    "print(\"Size of the clusters: \", model.summary.clusterSizes)\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictionsDF)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "print(\"SSE: \",model.computeCost(predictionsDF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the clusters:  [48, 11, 370, 431, 1663, 77, 16, 135, 986, 107]\n",
      "Silhouette with squared euclidean distance = 0.13511967145979883\n"
     ]
    }
   ],
   "source": [
    "# Cluster with GMM and Evaluate\n",
    "from pyspark.ml.clustering import GaussianMixture\n",
    "# Trains a GMM model.\n",
    "gmm = GaussianMixture(k=10,featuresCol=\"scaledFeatures\")\n",
    "modelGMM = gmm.fit(scaledDF)\n",
    "# Make predictions\n",
    "predictionsDFGMM = modelGMM.transform(scaledDF)\n",
    "\n",
    "print(\"Size of the clusters: \", modelGMM.summary.clusterSizes)\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluatorGMM = ClusteringEvaluator()\n",
    "silhouette = evaluatorGMM.evaluate(predictionsDFGMM)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
