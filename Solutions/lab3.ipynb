{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from datetime import datetime\n",
    "import math\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total line of the raw register RDD =  25319029\n",
      "Lines of the filted register RDD =  25104121\n"
     ]
    }
   ],
   "source": [
    "registerRDD_raw = sc.textFile(\"/data/students/bigdata_internet/lab3/register.csv\").map(lambda row:row.split(\"\\t\"))\n",
    "# print(registerRDD_raw.take(3))\n",
    "print(\"Total line of the raw register RDD = \",registerRDD_raw.count())\n",
    "def regFilter(e):\n",
    "    if(e[0]==\"station\"):\n",
    "        return False\n",
    "    if(e[2]=='0' and e[3]=='0'):\n",
    "        return False\n",
    "    return True\n",
    "registerRDD = registerRDD_raw.filter(regFilter)\n",
    "# print(registerRDD.take(3))\n",
    "print(\"Lines of the filted register RDD = \", registerRDD.count())\n",
    "\n",
    "stationsRDD= sc.textFile(\"/data/students/bigdata_internet/lab3/stations.csv\").map(lambda row:row.split(\"\\t\")).filter(lambda row:False if row[0]==\"id\" else True)\n",
    "# stationsRDD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', ['2.180019', '41.397978', 'Gran Via Corts Catalanes']),\n",
       " ('2', ['2.176414', '41.394381', 'Plaza TetuÃ¡n']),\n",
       " ('3', ['2.181164', '41.393750', 'Ali Bei'])]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transfrom stationsRDD to pair RDD\n",
    "stationsPairRDD = stationsRDD.map(lambda e:(e[0],[e[1],e[2],e[3]]))\n",
    "stationsPairRDD.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1-5-05', '0.162'), ('1-5-08', '0.015'), ('1-5-13', '0.023')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calcultate the criticality\n",
    "def registerRemap(row):\n",
    "    key = str(row[0])+\"-\"+datetime.strptime(row[1],\"%Y-%m-%d %H:%M:%S\").strftime(\"%w-%H\")\n",
    "    val=[0,1] #val[0] is the flag for cretical reading, val[1] is a counter for reading\n",
    "    if row[3]=='0':\n",
    "        val[0] = 1 #the station is critical for that reading\n",
    "    return (key,val)\n",
    "\n",
    "STPairedRegisterRDD = registerRDD.map(registerRemap)\n",
    "creticality=STPairedRegisterRDD.reduceByKey(lambda e1,e2:[e1[0]+e2[0],e1[1]+e2[1]]).mapValues(lambda v :format(v[0]/v[1],'.3f'))\n",
    "creticality.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('58-1-01', '0.624'), ('9-5-10', '0.613'), ('9-5-22', '0.626')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter the pairs greater than the threshold\n",
    "threshold = 0.6\n",
    "def thresholdFilter(tup):\n",
    "    (key,val)=tup\n",
    "    if float(val)>=threshold:\n",
    "        return True\n",
    "    return False\n",
    "creticalityFilted = creticality.filter(thresholdFilter)\n",
    "creticalityFilted.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('9-5-10', '0.613'), ('10-6-00', '0.622'), ('58-1-01', '0.624')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order the result by increasing criticality\n",
    "def swapKeyVal(tup):\n",
    "    (key,val)= tup\n",
    "    return (val,key)\n",
    "creticalityFiltedSorted = creticalityFilted.map(swapKeyVal).sortByKey().map(swapKeyVal)\n",
    "creticalityFiltedSorted.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of obtained critical pairs is:  5\n",
      "[['10', '2.185206', '41.384875', 'Saturday', '00', '0.622'], ['9', '2.185294', '41.385006', 'Friday', '10', '0.613'], ['9', '2.185294', '41.385006', 'Friday', '22', '0.626'], ['58', '2.170736', '41.377536', 'Monday', '01', '0.624'], ['58', '2.170736', '41.377536', 'Monday', '00', '0.632']]\n"
     ]
    }
   ],
   "source": [
    "# join the criticality with the station inforamtion and store\n",
    "def remapCreticality(tup):\n",
    "    (key,val)=tup\n",
    "    keyList = key.split(\"-\")\n",
    "    day = \"Sunday\"\n",
    "    if keyList[1]=='1':\n",
    "        day=\"Monday\"\n",
    "    elif keyList[1]=='2':\n",
    "        day=\"Tuesday\"\n",
    "    elif keyList[1]=='3':\n",
    "        day=\"Wednesday\"\n",
    "    elif keyList[1]=='4':\n",
    "        day =\"Thursday\"\n",
    "    elif keyList[1]=='5':\n",
    "        day =\"Friday\"\n",
    "    elif keyList[1]=='6':\n",
    "        day = \"Saturday\"\n",
    "    valList = []\n",
    "    valList.append(day)\n",
    "    valList.append(keyList[2])\n",
    "    valList.append(val)\n",
    "    return (keyList[0],valList)\n",
    "creticalityFiltedSortedJoined=creticalityFiltedSorted.map(remapCreticality).join(stationsPairRDD)\n",
    "\n",
    "def remapToResult(tup):\n",
    "    (key,(v1,v2))=tup\n",
    "    result = []\n",
    "    result.append(key)\n",
    "    result.append(v2[0])\n",
    "    result.append(v2[1])\n",
    "    result.append(v1[0])\n",
    "    result.append(v1[1])\n",
    "    result.append(v1[2])\n",
    "    return result\n",
    "ResultRDD = creticalityFiltedSortedJoined.map(remapToResult)\n",
    "print(\"The number of obtained critical pairs is: \",ResultRDD.count())\n",
    "print(ResultRDD.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(ResultRDD,[\"station\",\"station longitude\",\"station latitude\",\\\n",
    "                                     \"day of week\",\"hour\",\"criticality value\"])\n",
    "df.write.csv(\"./lab3/RDD_result.csv\",header=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of data is= 25319028\n",
      "Number of data after filtering= 25104121\n"
     ]
    }
   ],
   "source": [
    "df_register_raw = spark.read.load(\"/data/students/bigdata_internet/lab3/register.csv\",\n",
    "                             format=\"csv\",\n",
    "                             header=True,\n",
    "                             inferSchema=True,\n",
    "                             sep=\"\\t\")\n",
    "# df_register_raw.show(3)\n",
    "print(\"The total number of data is=\",df_register_raw.count())\n",
    "df_stations = spark.read.load(\"/data/students/bigdata_internet/lab3/stations.csv\",\n",
    "                             format=\"csv\",\n",
    "                             header=True,\n",
    "                             inferSchema=True,\n",
    "                             sep=\"\\t\")\n",
    "# df_stations.show(3)\n",
    "\n",
    "# fitering the uncorrect data\n",
    "df_register = df_register_raw.filter(\"used_slots!=0 or free_slots!=0 \")\n",
    "print(\"Number of data after filtering=\",df_register.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----+--------+\n",
      "|station|     day|hour|critical|\n",
      "+-------+--------+----+--------+\n",
      "|      1|Thursday|  12|       0|\n",
      "|      1|Thursday|  12|       0|\n",
      "|      1|Thursday|  12|       0|\n",
      "+-------+--------+----+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+---------+----+-----------+\n",
      "|station|      day|hour|criticality|\n",
      "+-------+---------+----+-----------+\n",
      "|      1|Wednesday|  19|      0.004|\n",
      "|      3|   Friday|   4|      0.230|\n",
      "|      6| Saturday|  22|      0.134|\n",
      "+-------+---------+----+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.udf.register(\"critical\",lambda x:1 if x==0 else 0)\n",
    "spark.udf.register(\"calCriticality\",lambda a,b:format(a/b,'.3f'))\n",
    "# calculate the critiality\n",
    "dfRemap = df_register.selectExpr(\"station\",\"date_format(timestamp,'EEEE') AS day\", \"date_format(timestamp,'H') As hour\",\"critical(free_slots) AS critical\" )\n",
    "dfRemap.show(3)\n",
    "dfRemap.createOrReplaceTempView(\"register\")\n",
    "\n",
    "dfSumCount = spark.sql(\"SELECT station,day,hour,sum(critical) AS numCritial,count(critical) AS totalNum FROM register GROUP BY station,day,hour\")\n",
    "dfCriticality = dfSumCount.selectExpr(\"station\",\"day\",\"hour\",\"calCriticality(numCritial,totalNum) AS criticality\")\n",
    "dfCriticality.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----+-----------+\n",
      "|station|     day|hour|criticality|\n",
      "+-------+--------+----+-----------+\n",
      "|     58|  Monday|   0|      0.632|\n",
      "|      9|  Friday|  10|      0.613|\n",
      "|     10|Saturday|   0|      0.622|\n",
      "|     58|  Monday|   1|      0.624|\n",
      "|      9|  Friday|  22|      0.626|\n",
      "+-------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter the data greater than the threshold\n",
    "threshold=0.6\n",
    "condition = \"criticality>=\"+str(threshold)\n",
    "dfSelected=dfCriticality.filter(condition)\n",
    "dfSelected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+----+-----------+\n",
      "|station|     day|hour|criticality|\n",
      "+-------+--------+----+-----------+\n",
      "|      9|  Friday|  10|      0.613|\n",
      "|     10|Saturday|   0|      0.622|\n",
      "|     58|  Monday|   1|      0.624|\n",
      "|      9|  Friday|  22|      0.626|\n",
      "|     58|  Monday|   0|      0.632|\n",
      "+-------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sorting\n",
    "dfSorted = dfSelected.sort(\"criticality\")\n",
    "dfSorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of obtained critical pairs is:  5\n",
      "+-------+---------+---------+--------+----+-----------+\n",
      "|station| latitude|longitude|     day|hour|criticality|\n",
      "+-------+---------+---------+--------+----+-----------+\n",
      "|      9|41.385006| 2.185294|  Friday|  10|      0.613|\n",
      "|     10|41.384875| 2.185206|Saturday|   0|      0.622|\n",
      "|     58|41.377536| 2.170736|  Monday|   1|      0.624|\n",
      "|      9|41.385006| 2.185294|  Friday|  22|      0.626|\n",
      "|     58|41.377536| 2.170736|  Monday|   0|      0.632|\n",
      "+-------+---------+---------+--------+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine two df\n",
    "df_result = dfSorted.join(df_stations,dfSorted.station==df_stations.id).select(\"station\",\"latitude\",\"longitude\",\"day\",\"hour\",\"criticality\")\n",
    "print(\"The number of obtained critical pairs is: \",df_result.count())\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result.write.csv(\"./lab3/Dataframe_result.csv\",header=True,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS TASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------+\n",
      "| id|                name|distance|\n",
      "+---+--------------------+--------+\n",
      "|  1|Gran Via Corts Ca...|   1.485|\n",
      "|  2|       Plaza TetuÃ¡n|   0.987|\n",
      "|  3|             Ali Bei|   1.201|\n",
      "+---+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#station distance calculation\n",
    "def distanceToCenter(lat2,long2):\n",
    "    toRad = 0.01745329252\n",
    "    lat1 = 41.386904*toRad\n",
    "    long1 =2.169989*toRad\n",
    "    lat2=lat2*toRad\n",
    "    long2=long2*toRad\n",
    "    r = 6356.725 # km\n",
    "    temp1 =math.sin((lat2-lat1)/2)*math.sin((lat2-lat1)/2)\n",
    "    temp2 = math.cos(lat1)*math.cos(lat2)*math.sin((long2-long1)/2)*math.sin((long2-long1)/2)\n",
    "    return format(2*r*math.asin(math.sqrt(temp1+temp2)),'.3f') \n",
    "spark.udf.register(\"distanceCal\",lambda lat,long:distanceToCenter(lat,long))\n",
    "dfStationDist = df_stations.selectExpr(\"id\",\"name\",\"distanceCal(latitude,longitude) AS distance\")\n",
    "dfStationDist.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|station|         avg_used|\n",
      "+-------+-----------------+\n",
      "|    148| 9.81105348279516|\n",
      "|    243|9.272743674168959|\n",
      "|     31|5.277462216695235|\n",
      "+-------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Average among reading of used slots\n",
    "df_register.createOrReplaceTempView(\"register2\")\n",
    "dfStationAvg = spark.sql(\"SELECT station, avg(used_slots) AS avg_used FROM register2 GROUP BY station\")\n",
    "dfStationAvg.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------+-------+-----------------+\n",
      "| id|            name|distance|station|         avg_used|\n",
      "+---+----------------+--------+-------+-----------------+\n",
      "| 65|Rambla Catalunya|   0.554|     65| 7.31260474158157|\n",
      "|126|          Girona|   1.179|    126|4.796550416136734|\n",
      "| 81|        Casanova|   1.464|     81|6.274346461792722|\n",
      "+---+----------------+--------+-------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+\n",
      "|               U1|\n",
      "+-----------------+\n",
      "|8.103784376830165|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStationNear = dfStationDist.filter(\"distance<1.5\").join(dfStationAvg,dfStationAvg.station==dfStationDist.id)\n",
    "dfStationNear.show(3)\n",
    "U1=dfStationNear.selectExpr(\"avg(avg_used) AS U1\" )\n",
    "U1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+-------+-----------------+\n",
      "| id|       name|distance|station|         avg_used|\n",
      "+---+-----------+--------+-------+-----------------+\n",
      "|148|Rambla Prim|   4.801|    148| 9.81105348279516|\n",
      "|243|  PalÃ¨ncia|   3.866|    243|9.272743674168959|\n",
      "| 31|  ProvenÃ§a|   1.570|     31|5.277462216695235|\n",
      "+---+-----------+--------+-------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------------+\n",
      "|               U2|\n",
      "+-----------------+\n",
      "|7.889224284652577|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStationFar = dfStationDist.filter(\"distance>=1.5\").join(dfStationAvg,dfStationAvg.station==dfStationDist.id)\n",
    "dfStationFar.show(3)\n",
    "U2=dfStationFar.selectExpr(\"avg(avg_used) AS U2\" )\n",
    "U2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: There are more used the station closer to center. U1=8.10  U2=7.89"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
