{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a simple Spark job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter notebook\n",
    "Code to execute in Pyspark(local) and Pyspark(YARN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum is: 46\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\") #load the data from file to rdd\n",
    "fields_rdd = rdd.map(lambda line: line.split(\",\")) # transform and split each line of the file into a list of string pair\n",
    "value_rdd = fields_rdd.map(lambda l: int(l[1])) #transfrom the age string into a list of int \n",
    "value_sum = value_rdd.reduce(lambda v1, v2: v1+v2)# use reduce to calcute the sum of the age\n",
    "print(\"The sum is:\", value_sum) #print the sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1: The printed value is 46, the propose is shown behine the code above\n",
    "\n",
    "A2: The inpurt file is on HDFS file system \n",
    "\n",
    "A3: It can still print the value of 46, but it takes more time to print after the execution. The Cluster manager interface is a graphcal interface, operations a based on click but not command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute in a pyspark shell\n",
    "Command to execute in the terminal:\n",
    "\n",
    "pyspark --master local --deploy-mode client\n",
    "\n",
    "The result is the same as ex1.1\n",
    "\n",
    "A: \"--master local\" means the spark executors is executed on the local machine(here is the gateway)\n",
    "\n",
    "    \"--deploy-mode client\" menans the driver is deployed locally(on the gateway)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark script and run it from the command line\n",
    "A: They are not in the same file system. My script is locate on the gateway file system. The file is located on the HDFS file system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Play with HDFS\n",
    "The command to copy the the HDFS file to local file system isï¼š\n",
    "\n",
    "s287513@jupyter-s287513:~$ hdfs dfs -copyToLocal /data/students/bigdata_internet/lab1/lab1_dataset.txt\n",
    "\n",
    "A1: No, it will not automatically affect the HDFS file.The command to copy a new create file form gateway to HDFS file system is :\n",
    "    \n",
    "    s287513@jupyter-s287513:~$ hdfs dfs -copyFromLocal ex2.txt \n",
    "    \n",
    "A2: The complete path of my file in HDFS is: hdfs://BigDataHA/user/s287513/ex2.txt\n",
    "    \n",
    "    On the gateway local file system is: /home/students/s287513/ex2.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Run a Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\")  #load the data from file to rdd\n",
    "age = rdd.map(lambda age_r: (age_r.split(',')[0],int(age_r.split(',')[1])))  # transform and split each line of the file into a list of string and int pair\n",
    "total =age.reduceByKey(lambda v1,v2:v1+v2) # use the reduceByKey command to get the total age of each name\n",
    "total.saveAsTextFile(\"lab1/ex3.txt\") # save the result in the HDFS in a .txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A1 : As the code above shows\n",
    "    \n",
    "A2 : The output folder is a folder with the name we save the result(here is ex3.txt), and containing different parts of the result. The reason why there are multiple parts in the folder is that when spark is running, it split the input data into different partitions, and the result of each partion will be saved as a part of the final result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.textFile(\"/data/students/bigdata_internet/lab1/lab1_dataset.txt\") #load the data from file to rdd\n",
    "age = rdd.map(lambda age_r: (age_r.split(',')[0],age_r.split(',')[1])) # transform and split each line of the file into a list of string pair\n",
    "total =age.reduceByKey(lambda v1,v2:v1+\":\"+v2) # use the reduceByKey command to join the age of each name\n",
    "total.saveAsTextFile(\"lab1/ex4.txt\") # save the result in the HDFS in a .txt file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
